{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "oLG0gbWnVw4l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7f95150-8caa-4984-bc04-6607375d9042"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "import gensim\n",
        "from nltk import word_tokenize\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQWI0rZYmmIX",
        "outputId": "36a44960-3cfd-4e05-e7f3-0582ad976243"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get the cosine similarity between a relation and query\n",
        "# Note: Be sure to prepend the relation with ns:\n",
        "word2vec_model = gensim.models.Word2Vec.load('/content/drive/MyDrive/UML/HW6/knowledge-graph/word2vec_train_dev.dat')\n",
        "def get_rel_score_word2vecbase(rel, query):\n",
        "    if rel not in word2vec_model.wv:\n",
        "        return 0.0\n",
        "    words = word_tokenize(query.lower())\n",
        "    w_embs = []\n",
        "    for w in words:\n",
        "        if w in word2vec_model.wv:\n",
        "            w_embs.append(word2vec_model.wv[w])\n",
        "    return np.mean(cosine_similarity(w_embs, [word2vec_model.wv[rel]]))\n",
        "\n",
        "\n",
        "# Function to load the graph from file\n",
        "def load_graph():\n",
        "    # Preparing the graph\n",
        "    graph = defaultdict(list)\n",
        "    for line in open('graph'):\n",
        "        line = eval(line[:-1])\n",
        "        graph[line[0]].append([line[1], line[2]])\n",
        "    return graph\n",
        "\n",
        "\n",
        "# Function to load the queries from file\n",
        "# Preparing the queries\n",
        "def load_queries():\n",
        "    queries = []\n",
        "    for line in open('annotations'):\n",
        "        line = eval(line[:-1])\n",
        "        queries.append(line)\n",
        "    return queries"
      ],
      "metadata": {
        "id": "-jQQrxRhV0r5"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph = load_graph()"
      ],
      "metadata": {
        "id": "Sr3VKQ8lurm1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queries = load_queries()"
      ],
      "metadata": {
        "id": "ozxiHf-rS3gA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get actual answers for the current query for evaluation\n",
        "\n",
        "actual_ans = []\n",
        "\n",
        "for i in range(len(queries)):\n",
        "    current_ans = queries[i][5] \n",
        "    ans = []\n",
        "    for j in range(len(current_ans)):\n",
        "        ans.append(current_ans[j]['AnswerArgument'])\n",
        "\n",
        "    actual_ans.append(ans)"
      ],
      "metadata": {
        "id": "t2GbmfYXS7vx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 0.25\n",
        "predicted_ans = []\n",
        "\n",
        "for i in range(len(queries)):\n",
        "    \n",
        "    #Get Current Query, answer entities and starting node\n",
        "    current_qs = queries[i][1]\n",
        "    starting_node = queries[i][2]\n",
        "    \n",
        "    #Initialize lists to hold Queues and predicted answers\n",
        "    pred_ans = []\n",
        "    queue = []\n",
        "    visited_nodes = []\n",
        "    \n",
        "    #Enqueue the current node to the queue\n",
        "    visited_nodes.append(starting_node)\n",
        "    queue.append(starting_node)\n",
        "       \n",
        "    #Run till queue is not empty\n",
        "    while len(queue) != 0:\n",
        "\n",
        "        curr_node = queue.pop(0)\n",
        "\n",
        "        for k in range(len(graph[curr_node])):\n",
        "            neighbor_node = graph[curr_node][k][1]    \n",
        "            relation = \"ns:\" + str(graph[curr_node][k][0])\n",
        "            cos_sim = get_rel_score_word2vecbase(relation, current_qs)\n",
        "            if neighbor_node not in visited_nodes:\n",
        "                visited_nodes.append(neighbor_node)\n",
        "                if cos_sim > threshold:\n",
        "                    pred_ans.append(neighbor_node)\n",
        "                else:\n",
        "                    queue.append(neighbor_node)\n",
        "    \n",
        "    #Hold Predicted answers for each query\n",
        "    predicted_ans.append(pred_ans)"
      ],
      "metadata": {
        "id": "ZE0xH01LUknR"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(y_pred, y_true):\n",
        "    \n",
        "    #Calculate Precision\n",
        "    if len(y_pred) == 0:\n",
        "        precision = 0\n",
        "    else:\n",
        "        count = 0\n",
        "        for answer in y_pred:\n",
        "            if answer in y_true:\n",
        "                count += 1\n",
        "        precision = count/len(y_pred)\n",
        "\n",
        "    #Calculate Recall\n",
        "    if len(y_true) == 0:\n",
        "        recall = 0\n",
        "    else:\n",
        "        count = 0\n",
        "        for answer in y_true:\n",
        "            if answer in y_pred:\n",
        "                count += 1\n",
        "        recall = count/len(y_true)\n",
        "\n",
        "    if (precision + recall) == 0:\n",
        "        f1_score = 0\n",
        "    else:\n",
        "        f1_score = 2 * precision * recall / (precision + recall)\n",
        "    \n",
        "    return precision, recall, f1_score"
      ],
      "metadata": {
        "id": "AHn5C69zWmrz"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "\n",
        "precision = 0\n",
        "recall = 0\n",
        "f1_score = 0\n",
        "\n",
        "for i in range(len(predicted_ans)):\n",
        "    precision += evaluate(predicted_ans[i], actual_ans[i])[0]\n",
        "    recall += evaluate(predicted_ans[i], actual_ans[i])[1]\n",
        "    f1_score += evaluate(predicted_ans[i], actual_ans[i])[2]\n",
        "    \n",
        "precision = precision / len(predicted_ans)\n",
        "recall = recall / len(predicted_ans)\n",
        "f1_score = f1_score / len(predicted_ans)\n",
        "\n",
        "print('Precision:' , round(precision,4))\n",
        "print('Recall:', round(recall,4))\n",
        "print('F1-Score:', round(f1_score,4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbLq8Q-cXzkI",
        "outputId": "5184a564-fb0a-4633-aefc-09e36ce2a0fe"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.1572\n",
            "Recall: 0.5505\n",
            "F1-Score: 0.206\n"
          ]
        }
      ]
    }
  ]
}